# The Lexer

> In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).
> 
> [Wikipedia - Lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis)
